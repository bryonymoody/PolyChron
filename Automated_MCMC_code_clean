#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug  7 20:54:35 2020

@author: bryony
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import random
from statistics import mean 
import time
import pymc3 as pm3
from scipy.sparse import csr_matrix


############### TO DO LIST #######
# automate checking of thetas and phis
# output historgrams with associated trace plots
# output table of HPD intervals

 
#import the calibration curve 
calibration = pd.read_csv('spline_interpolation_new.txt', delim_whitespace=True)
calibration_data = calibration.to_dict()
#np.random.seed(9001)
#random.seed(9001)


def sampling_vec(post_phase, prev_phase):
    track = []
    track_vec = []
    for i in range(len(post_phase)):
        h = prev_phase[i]
        f = post_phase[i]        
        if h == "abuting":
            track.append("combined")
        elif h == "overlap":
            if prev_phase[i-1]  == "overlap":
                track.append("overlap_upper") 
            else:
                track.append("upper")
            track_vec.append(i)      
        else:
            track.append("upper")
            track_vec.append(i) 
            
                    
        if f == "abuting":
            pass
        elif f == "overlap":
            if post_phase[i+1]  == "overlap":
                track.append("overlap_beta")
            else:
                track.append("lower")
                         
        else:
            track.append("lower")  
        track_vec.append(i)
    return [track, track_vec]


def phase_limits(phases):
    upper = []
    lower = []
    i = 0
    for j in range(len(post_phase)):
        #print(i)
        upper.append(phases[i])
        i = i + 1
        lower.append(phases[i])
        if post_phase[j] == "end":
            i = i
           # print("testend")
        elif post_phase[j] != "abuting":    
                i = i + 1
             #   print("teest")                
    phase_bounds = [list(x) for x in zip(lower, upper)]    
    return phase_bounds   


def accept_prob(accept, phi_accept):
    probs = []
    for i in range(len(accept)):
        probs.append(len(accept[i])/30000)
    for j in range(len(phi_accept)):
        probs.append(len(phi_accept[j])/30000)
    return np.array(probs)


def likeli(x, s, theta):
    value_theta = math.exp(-((x-calibration_data["Carbon_year"][theta])**2)/(2*(s**2 + calibration_data["Carbon_error"][theta]**2)))
    return value_theta

x = 19000
#finds likelihood values of multiple thetas
#def likelihood_func(x, s, A, P):
# #   t0 = time.perf_counter
#    likelihood_vec = []
#    if  1500 < x < 35000:        
#        for theta in range(A, P):  
#            value = likeli(x, s, theta)
#            likelihood_vec.append(value)        
#        theta = np.array(range(A, P))
#        intep_theta = np.linspace(A, P, ((P-A)*10)+1)
#
#      
#    elif x < 1500: 
#        for theta in range(0, x+6500):
#            value = likeli(x, s, theta)
#            likelihood_vec.append(value)
#        theta = np.array(range(6500))        
#        intep_theta = np.linspace(0, 6500, 65001)
#    else:
#        for theta in range(35000, 50001):    
#            value = likeli(x, s, theta)
#            likelihood_vec.append(value)
#        intep_theta = np.linspace(35000, 50000, 65001) 
#        theta = np.array(range(350000, 50001))        
#    interp_prob = np.interp(intep_theta, theta, likelihood_vec)
#    prob = interp_prob/sum(interp_prob) 
#
#    lhood_df = [intep_theta, prob]    
#    return lhood_df

def likelihood_func(x, s, A, P):
    likelihood_vec = []    
    for theta in range(A, P):  
        value = likeli(x, s, theta)
        likelihood_vec.append(value)        
    theta = np.array(range(A, P))
    intep_theta = np.linspace(A, P, ((P-A)*10)+1)
    interp_prob = np.interp(intep_theta, theta, likelihood_vec)
    prob = interp_prob/sum(interp_prob) 

    lhood_df = [intep_theta, prob]    
    return lhood_df

def dict_seek(i, key, site_dict):
   #  print(i)
   #  print(key)
     phase_len = site_dict[key]["boundaries"][1] - site_dict[key]["boundaries"][0]           
     theta = site_dict[key]["dates"][i][0]          
     llhood = site_dict[key]["dates"][i][1]       
     like1 = llhood[0:][0]      
     ind = np.where(like1 == int(theta))[0][0]         
     v1 = like1[ind:ind+11]        
     res = min(enumerate(v1), key=lambda x: abs(theta - x[1]))       
     c = np.where(like1 == res[1])                
     x_temp = llhood[1:][0][c[0]][0]     
     x = x_temp/phase_len

     return x

        
def post_h_theta_phi(site_dict, R, phis):

    h = [dict_seek(j, i, site_dict)
       for i in site_dict.keys()
       for j in range(len(site_dict[i]["dates"]))]
    
    return h 




def theta_finder(theta, llhood):
    v_temp = llhood.Theta[math.floor(theta) <= llhood.Theta]
    v = v_temp[v_temp <= math.floor(theta) + 1]
    res = min(enumerate(v), key=lambda x: abs(theta - x[1]))
    return res



def strat_rel(site_dict, key):
    upstrat = site_dict[key]["dates"][i][3][0]
    lowstrat = site_dict[key]["dates"][i][3][1]
    a = [contextno.index(i) for i in upstrat]
    b = [contextno.index(i) for i in lowstrat] 
    stratup = min([thetas[i] for i in a])
    stratlow = max([thetas[i] for i in b])                       
    return [stratup, stratlow]    
     
def post_h_theta_phi_ordered(site_dict, R, phis):    
    h = []
    for i in range(len(site_dict.keys())):
            
        key = list(site_dict.keys())[i]
        for i in range(len(site_dict[key]["dates"])): 
             upper = site_dict[key]["boundaries"][1]
             lower = site_dict[key]["boundaries"][0]
             phase_len =  upper - lower #phase length                       
             stratup = strat_rel(site_dict, key)[1]
             stratlow = strat_rel(site_dict, key)[0]
             theta = site_dict[key]["dates"][i][0] #gets each date and likelihood in the dictionary
             llhood = site_dict[key]["dates"][i][1]
             temp_vec = llhood[llhood.Theta <= upper]
             temp_vec_1 = temp_vec.Probability[temp_vec.Theta > lower]
             sum_prob = np.sum(temp_vec_1)
             temp_vec_2 = llhood[llhood.Theta <= stratup]
             temp_vec_3 = temp_vec_2.Probability[temp_vec_2.Theta > stratlow]
             sum_prob_1 = np.sum(temp_vec_3)
             res = theta_finder(theta, llhood)
             x_temp = llhood.Probability[llhood.Theta == res[1]].item()*(sum_prob/sum_prob_1)#finds probability of that theta
             x = x_temp/phase_len #divides bphase length   
             h.append(x)

    return h 
 

def dict_seek_4(i, key, site_dict):   
 #    print(i)
 #    print(key)        
     phase_len = site_dict[key]["boundaries"][1] - site_dict[key]["boundaries"][0]           
     theta = site_dict[key]["dates"][i][0]          
     llhood = site_dict[key]["dates"][i][1]  
     
     ind = np.where(llhood[0:][0] == int(theta))[0][0]
     v1 = llhood[0:][0][ind:ind+11]             
     res = min(enumerate(v1), key=lambda x: abs(theta - x[1]))       
     c = np.where(llhood[0:][0] == res[1])              
     x_temp = llhood[1:][0][c[0]][0]      
     x = x_temp/phase_len 
  
     return x_temp, x
 

def post_h_theta_phi_4(site_dict, R):
    h = [dict_seek_4(j, i, site_dict)
       for i in site_dict.keys()
       for j in range(len(site_dict[i]["dates"]))]  
    h1 = [i[0] for i in h]
    h2 = [i[1] for i in h] 
    return h1, h2    

def dict_update(test_dict_2, post_thetas, post_phis, inter, inter2):    
    count = 0
    count1 = 0
    b_lst = phase_limits([phi[inter2] for phi in post_phis])
    for i2 in phiref:
        test_dict_2[i2]["boundaries"] = b_lst[count1]
        count1 = count1 + 1  
        for j in range(len(test_dict_2[i2]["dates"])):
            test_dict_2[i2]["dates"][j][0]  = post_thetas[count][inter]
            count = count + 1
    return test_dict_2     

#sampling functions

def upp_samp_1(A, P, thetas, phis, M, m, keyref):
    limits = [max([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]), P]
    return limits

def upp_samp_2(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m+2])
    limits = [max(in_phase_dates), P]
    return limits

def upp_samp_3(A, P, thetas, phis, M, m, keyref):
    limits = [max([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]), min([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m-1]]])]
    return limits

def upp_samp_4(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m-1]]]
    in_phase_dates.append(phis[m+2])    
    limits = [max(in_phase_dates), min([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]])]
    return limits


def upp_samp_5(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m-1])    
    limits = [max(in_phase_dates), phis[m-2]]
    return limits

def upp_samp_6(A, P, thetas, phis, M, m, keyref):
    limits = [max([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]), phis[m-1]]
    return limits

def upp_samp_7(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m+2])
    limits = [max(in_phase_dates), phis[m-1]]
    return limits

def upp_samp_8(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m-1])    
    limits = [max(in_phase_dates), phis[m-3]]
    return limits

def low_samp_1(A, P, thetas, phis, M, m, keyref):
    limits = [max([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m+1]]]), min([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]])]
    return limits

def low_samp_2(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m-2])
    limits = [max([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m+1]]]), min(in_phase_dates)]
    return limits

def low_samp_3(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m+1])
    limits = [phis[m+2], min(in_phase_dates)]
    return limits

def low_samp_4(A, P, thetas, phis, M, m, keyref):
    limits = [phis[m+1], min([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]])]
    return limits

def low_samp_5(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m-2])
    limits = [phis[m+1], min(in_phase_dates)]
    return limits

def low_samp_6(A, P, thetas, phis, M, m, keyref):
    limits = [A, min([thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]])]
    return limits

def low_samp_7(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m-2])
    limits = [A, min(in_phase_dates)]
    return limits

def overlap_samp_1(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m-1])    
    limits = [max(in_phase_dates), phis[m-3]]
    return limits

def overlap_samp_2(A, P, thetas, phis, M, m, keyref):
    in_phase_dates = [thetas[i] for i, j in enumerate(keyref) if j == phiref[samp_vec_track[m]]]
    in_phase_dates.append(phis[m+1])      
    limits = [phis[m+3], min(in_phase_dates)]
    return limits

#sampling dictionary
phi_samp_dict = {
        'upper' : {
                'start':{
                        'abuting':upp_samp_1, 
                        'overlap':upp_samp_2, 
                        'gap':upp_samp_1, 
                        'end':upp_samp_1, 
                        }, 
                'abuting':{
                        'abuting':upp_samp_3, 
                        'overlap':upp_samp_4, 
                        'gap':upp_samp_3, 
                        'end':upp_samp_3}, 
                'overlap':{
                        'abuting':upp_samp_5, 
                        'overlap':upp_samp_5, 
                        'gap':upp_samp_5, 
                        'end':upp_samp_5
                        }, 
                'gap':{
                        'abuting':upp_samp_6, 
                        'overlap':upp_samp_7,  
                        'gap':upp_samp_6,
                        'end':upp_samp_6}
                        },
        'lower' :  {
                'start':{
                        'abuting':low_samp_1, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_4, 
                        'end':low_samp_6
                        }, 
                'abuting':{
                        'abuting':low_samp_1, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_4, 
                        'end':low_samp_6
                        }, 
                'overlap':{
                        'abuting':low_samp_2, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_5, 
                        'end':low_samp_7
                        }, 
                'gap':{
                        'abuting':low_samp_1, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_4, 
                        'end':low_samp_6
                        }
                },
        'combined' :  {
                'start':{
                        'abuting':low_samp_1, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_4, 
                        'end':low_samp_6
                        }, 
                'abuting':{
                        'abuting':low_samp_1, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_4, 
                        'end':low_samp_6
                        }, 
                'overlap':{
                        'abuting':low_samp_2, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_5, 
                        'end':low_samp_7
                        }, 
                'gap':{
                        'abuting':low_samp_1, 
                        'overlap':low_samp_3, 
                        'gap':low_samp_4, 
                        'end':low_samp_6
                        }
                },     

        'overlap_upper' :  {
                'start':{
                        'abuting':overlap_samp_1, 
                        'overlap':overlap_samp_1, 
                        'gap':overlap_samp_1, 
                        'end':overlap_samp_1, 
                        }, 
                'abuting':{
                        'abuting':overlap_samp_1,  
                        'overlap':overlap_samp_1,  
                        'gap':overlap_samp_1, 
                        'end':overlap_samp_1, 
                        }, 
                'overlap':{
                        'abuting':overlap_samp_1,  
                        'overlap':overlap_samp_1, 
                        'gap':overlap_samp_1,  
                        'end':overlap_samp_1, 
                        }, 
                'gap':{
                        'abuting':overlap_samp_1,  
                        'overlap':overlap_samp_1, 
                        'gap':overlap_samp_1,  
                        'end':overlap_samp_1, 
                        }
                },                  
        'overlap_beta' :  {
                'start':{
                        'abuting':overlap_samp_2, 
                        'overlap':overlap_samp_2, 
                        'gap':overlap_samp_2, 
                        'end':overlap_samp_2, 
                        }, 
                'abuting':{
                        'abuting':overlap_samp_2,  
                        'overlap':overlap_samp_2,  
                        'gap':overlap_samp_2, 
                        'end':overlap_samp_2, 
                        }, 
                'overlap':{
                        'abuting':overlap_samp_2,  
                        'overlap':overlap_samp_2, 
                        'gap':overlap_samp_2,  
                        'end':overlap_samp_2, 
                        }, 
                'gap':{
                        'abuting':overlap_samp_2,  
                        'overlap':overlap_samp_2, 
                        'gap':overlap_samp_2,  
                        'end':overlap_samp_2, 
                        }
                }

        }


#inputs to the code

strat_vec = [[[1], [4,7]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]]]#, [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]], [[0], [0,0]]]

##shag river 
#A = 18000
#P = 27000
#
#rcd_est = [19320, 19380, 19050, 19510, 19410, 19150, 19280, 18920, 18860, 18590, 19020, 19180, 18730, 18620, 18660, 18640, 18730]
#rcd_err = [100, 100, 100, 110, 100, 110, 120, 110, 110,110, 110, 110, 110, 100, 100, 70, 110]
#
#theta_inits = [22050, 22150, 22100, 22200, 22400, 22300, 22600, 22850, 23100,23200,23300,23650,23930,23950,23800,23850,23900]
#phase_boundary_inits= [24000, 23750, 23500, 23250, 23000, 22750, 22500, 22250, 22000]  
#
#keyref = [8, 8, 8, 8, 8, 7, 6, 6, 5, 4, 3, 2, 2, 1, 1, 1, 1]
#phiref = [8, 7, 6, 5 ,4, 3, 2, 1]
#contextno=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11, 12,13,14,15,16,17]
#
#prev_phase = ["start", "abuting", "abuting", "abuting", "abuting", "abuting", "abuting", "abuting"]
#post_phase = ["abuting", "abuting", "abuting", "abuting", "abuting", "abuting", "abuting", "end"]
#
#
##betta simulated data
#A = 7000
#P = 13000
#
#rcd_est = [9205, 9177, 9139, 9045, 9010, 8962, 8926, 8894, 8882, 8857, 8906, 8878]
#rcd_err = [15,15, 15, 15, 15, 15, 14, 15, 14, 13, 13, 15]
#
#theta_inits = [10173, 10031, 10007, 9968, 9763, 9758, 9768, 9756, 9713, 9324, 9294, 9272] 
#phase_boundary_inits= [10300, 9965, 9350, 8000]
#
#keyref = [3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1]
#phiref = [3, 2, 1]
#contextno=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11, 12]
#
#prev_phase = ["start", "abuting", "abuting"]
#post_phase = ["abuting", "abuting", "end"]

#bryony simulated data
A = 20000
P = 30000


rcd_est = [20689, 20584, 20223, 19685, 19282, 18890]
rcd_err = [75, 75, 75, 75, 75, 75]

theta_inits = [25000, 24900, 24700, 24000, 23200, 22750] 
phase_boundary_inits= [25500, 24800, 24850, 23500, 23600, 22100]

keyref = [3, 3, 2, 2, 1, 1]
phiref = [3, 2, 1]
contextno=[1, 2, 3, 4, 5, 6]

prev_phase = ["start", "overlap", "overlap"]
post_phase = ["overlap", "overlap", "end"]


##############################################
##initiating  likelihoods
rcds = [list(x) for x in zip(rcd_est, rcd_err)]
result = [likelihood_func(date[0], date[1], A, P) for date in rcds]


#initiating the inital dict
new_lst = [list(x) for x in zip(theta_inits, result, contextno, strat_vec)]
keys_lst = [list(x) for x in zip(keyref, new_lst)]
keys = list(set(keyref))
cvb = phase_limits(phase_boundary_inits)       
phis = phase_boundary_inits
thetas = theta_inits

track = []
keystrack = []
for d in phiref:
    ind = [index for index, element in enumerate(keyref) if element == d]
    test_lst = [new_lst[d] for d in ind]
    bound = cvb[phiref.index(d)]
   # strat = strat_vec
    track.append({'dates': test_lst, 'boundaries': bound, 'prev_phase_rel': prev_phase[phiref.index(d)], 'post_phase_rel': post_phase[phiref.index(d)]})
    keystrack.append(d)
test_dict_1 = dict(zip(keystrack, track)) 

site_dict_test_1 = test_dict_1
site_dict_test_2 = test_dict_1 
site_dict_test_3 = test_dict_1
site_dict_test_4 = test_dict_1
site_dict_test_5 = test_dict_1
###########################
        
K = len(thetas)
M = len(phis)


S = 20 #max(s_1, s_2, s_3, s_4, s_5, s_6, s_7)
R = P - A
post_s = []
step_1 = sampling_vec(post_phase, prev_phase)[0]  
samp_vec_track = sampling_vec(post_phase, prev_phase)[1]
step_2 = [prev_phase[i] for i in samp_vec_track]
step_3 = [post_phase[i] for i in samp_vec_track]

################################################################





########################################
#testing only!!!

#A = -2
#P = 14
#
#prev_phase = ["start", "abuting","gap"]
#post_phase = ["abuting","gap", "end"]
#
#step_1 = sampling_vec(post_phase, prev_phase)[0]  
#samp_vec_track = sampling_vec(post_phase, prev_phase)[1]
#step_2 = [prev_phase[i] for i in samp_vec_track]
#step_3 = [post_phase[i] for i in samp_vec_track]
#
#
#thetas = [10, 11, 7 ,6, 2, 3] 
#keyref = [3, 3,2, 2, 1, 1]
#phis = [12 ,9, 5,4, 1]
#M = len(phis)#m= random.sample(range(0,M), 1)[0]  
#
#for m in range(0,M):    
#    print(m)
#    print(phi_samp_dict[step_1[m]][step_2[m]][step_3[m]](A, P, thetas, phis, M, m, keyref))
################################################
#
##m= random.sample(range(0,M), 1)[0]  


    
#set up empty post vectors
accept = [[] for _ in range(len(thetas))]    
phi_accept = [[] for _ in range(len(phis))]     

post_thetas = [[] for _ in range(len(thetas))] 
post_phis = [[] for _ in range(len(phis))]     

for p in range(len(thetas)):
        post_thetas[p].append(theta_inits[p])

for j in range(len(phis)):
        post_phis[j].append(phase_boundary_inits[j])        

prev_prob_test = post_h_theta_phi_4(site_dict_test_1, R)[1]

for l in np.linspace(0, 30000, 10001):
    t0 = time.perf_counter()
    i = int(l)
    print("i " + str(i) + "\n")

#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <=db <= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break    
    site_dict_test_1 = dict_update(site_dict_test_1, post_thetas, post_phis, i, i)

   #step 1 #####################

    k = random.sample(range(0,K), 1)[0]
    oldtheta = thetas[k]
    thetas[k] = np.random.uniform(site_dict_test_1[keyref[k]]["boundaries"][1], site_dict_test_1[keyref[k]]["boundaries"][0])

#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break

    for g in range(len(thetas)):
        post_thetas[g].append(thetas[g])

    site_dict_test_2 = dict_update(site_dict_test_2, post_thetas, post_phis, i+1, i)   
    prob_1_test = post_h_theta_phi(site_dict_test_2, R, phis)

    c_test = []
    for j in range(len(prob_1_test)):
        c_test.append(prob_1_test[j]/prev_prob_test[j])
    h =  np.prod(c_test)
    
       
    if h >= 1 :
        accept[k].append(thetas[k])
    else:
        u = np.random.uniform(0, 1)    
        if h > u :
            accept[k].append(thetas[k])
        else:
            thetas[k] = oldtheta
            for g2 in range(len(thetas)):
                post_thetas[g2][i+1] = post_thetas[g2][i]

            prob_1_test = prev_prob_test

    #step 2 #############################
#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break
   
    m= random.sample(range(0,M), 1)[0]    
    s1 = max(phis) - min(phis) 
    f_phi1 = (s1**(1-M))/(R-s1)
    
    oldphi = phis[m]
#    print(tester(phis, m))    
#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break
  
    lims = phi_samp_dict[step_1[m]][step_2[m]][step_3[m]](A, P, thetas, phis, M, m, keyref) 
    phis[m] = np.random.uniform(lims[0], lims[1])
 
    for v1 in range(len(phis)):
        post_phis[v1].append(phis[v1])
         
    s2 = max(phis) - min(phis)  
    
#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break
    
    f_phi2 = (s2**(1-M))/(R-s2)       
    
    site_dict_test_3 = dict_update(site_dict_test_3, post_thetas, post_phis, i+1, i+1)        
    prob_2_test = post_h_theta_phi(site_dict_test_3, R, phis)

    c_test = []
    for k in range(len(prob_2_test)):
        c_test.append(prob_2_test[k]/prob_1_test[k])
    h = np.prod(c_test)*(f_phi2/f_phi1)
   
    if h >= 1 :
        phi_accept[m].append(phis[m])
        if m == M-1:
            post_s.append(max(phis) - phis[m])
        elif m == 0:
            post_s.append(phis[m]-min(phis))
    else:
        u = np.random.uniform(0, 1)
        if h > u :
            phi_accept[m].append(phis[m])
            if m == M-1:
                post_s.append(max(phis) - phis[m])
            elif m == 0:
                post_s.append(phis[m]- min(phis))        
        else:
            for v2 in range(len(phis)):
                post_phis[v2][i+1] = post_phis[v2][i]   
            phis[m] = oldphi
            prob_2_test = prob_1_test
     
########### DONT NEED ANYTHING FROM HERE CHANGING TO ACCOUNT FOR DIFFERENT MODELS SINCE IT'S JUST A SCALE AND SHIFT #####################

  # step3 ################
#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break
    

    step = np.random.uniform(-1*S, S)

    thetas = [x + step for x in thetas]
    phis = [x + step for x in phis] 
    
    for g2 in range(len(thetas)):
        post_thetas[g2].append(thetas[g2])
        
    for v2 in range(len(phis)):
        post_phis[v2].append(phis[v2]) 

    site_dict_test_4 = dict_update(site_dict_test_4, post_thetas, post_phis, i+2, i+2)
           
    temp = post_h_theta_phi_4(site_dict_test_4, R)
    prob_3_test = temp[1]

    b_test = temp[0] 

    c_test = []
    for f in range(len(prob_3_test)):
        c_test.append(prob_3_test[f]/prob_2_test[f])
    h = np.prod(c_test)  


    if h >= 1 :
        for j in range(len(phis)):
            phi_accept[j].append(phis[j])
        for k in range(len(thetas)):
            accept[k].append(thetas[k]
            )
        post_s.append(max(phis) - min(phis))        
    else:
        u = np.random.uniform(0,1)
        if h > u:
            for j in range(len(phis)):
                phi_accept[j].append(phis[j])
            for k in range(len(thetas)):
                accept[k].append(thetas[k]
                )
            post_s.append(max(phis) - min(phis))   
        else:
   
            for g2 in range(len(thetas)):
                post_thetas[g2][i+2] = post_thetas[g2][i+1]
                
            for v2 in range(len(phis)):
                post_phis[v2][i+2] = post_phis[v2][i+1]    
            thetas = [x - step for x in thetas]
            phis = [x - step for x in phis]
            prob_3_test = prob_2_test

            

    ##step 4 ##########
#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break

    rho = np.random.uniform(4/5, 5/4)
    constant = (rho-1)*mean(np.concatenate([phis, thetas])).item()
    thetas = [x*rho - constant for x in thetas]
    phis = [x*rho - constant for x in phis] 
    
    for s in range(len(thetas)):
        post_thetas[s].append(thetas[s])
        
    for e in range(len(phis)):
        post_phis[e].append(phis[e])        

    site_dict_test_5 = dict_update(site_dict_test_5, post_thetas, post_phis, i+3, i+3)
    s = max(phis) - min(phis)
    
    const = (R - s)/((R-(rho*s))*rho)   
    temp_2 = post_h_theta_phi_4(site_dict_test_5, R)
    a_test = temp_2[0] 
    prev_prob_test = temp_2[1]
 
    c_test = []
    for q in range(len(a_test)):
        c_test.append(a_test[q]/b_test[q])
    h_temp_test = np.prod(c_test)    
    h = h_temp_test*const

    if h >= 1 :
        for j in range(len(phis)):
            phi_accept[j].append(phis[j])
        for k in range(len(thetas)):
            accept[k].append(thetas[k]
            )
        post_s.append(max(phis) - min(phis))    
    else:

        u = np.random.uniform(0,1)
        #print(u)
        if h > u:
            for j in range(len(phis)):
                phi_accept[j].append(phis[j])
            for k in range(len(thetas)):
                accept[k].append(thetas[k]
                )
            post_s.append(max(phis) - min(phis))    

        else:            
            for g2 in range(len(thetas)):
                post_thetas[g2][i+3] = post_thetas[g2][i+2]
                
            for v2 in range(len(phis)):
                post_phis[v2][i+3] = post_phis[v2][i+2]   

            
            thetas = [(x + constant)/rho for x in thetas]
            phis = [(x + constant)/rho for x in phis] 
            prev_prob_test = prob_3_test
#    test_v1 = []
#    for da in thetas[0:4]:
#       test_v1.append( phis[1] <=da <= phis[0])
#       
#    test_v2 = []
#    for db in thetas[4:9]:
#       test_v2.append( phis[2] <= db<= phis[1])
#
#    test_v3 = []
#    for dc in thetas[9:12]:
#       test_v3.append( phis[3] <=dc <= phis[2])       
#    if np.prod(test_v1)*np.prod(test_v2)*np.prod(test_v3) == 0:
#        break

    tot = time.perf_counter() - t0
    print("tot " + str(tot))    
     
#plot code  
           


check = accept_prob(accept, phi_accept)
print(len(check[(0.2 >= check) & (check >= 0.4)]))

#k, bins, patches = plt.hist(post_s[100:], bins= np.linspace(0,2000, 201), color='#0504aa',
#          alpha=0.7, rwidth=0.85, density = True )
#plt.title('Simulated data using the Alpha-beta \'squeezing\' model')
#plt.xlabel('Phase length (calendar years)')
#plt.ylabel('Frequency')
#plt.ylim(0,0.0043)
#plt.xlim(0, 2000)
#plt.savefig("hist_sim_4.png")
#plt.plot(post_s)
#plt.savefig("trace_sim_4.png")

#k, bins, patches = plt.hist(accept[11], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[10], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[9], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[8], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[7], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[6], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#%%
#k, bins, patches = plt.hist(accept[5], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[4], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[3], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[2], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[1], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(accept[0], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )

#k, bins, patches = plt.hist(phi_accept[3], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(phi_accept[3], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(phi_accept[2], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(phi_accept[1], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(phi_accept[1], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
#k, bins, patches = plt.hist(phi_accept[0], bins='auto', color='#0504aa',
#                      alpha=0.7, rwidth=0.85, density = True )
plots =[0,1,2,3,4,5]
figure, axes = plt.subplots(nrows=6, ncols=1, sharex=True, sharey=True, figsize=[10,6])
plt.xlim(28000, 20000)
plt.ylim(0,0.003)
#for j in range(len(axes.ravel())):
for i, j in zip(plots,range(len(axes.ravel()))):
    axes[j].hist(accept[i], bins='auto', color='#0504aa',
                      alpha=0.7, rwidth=0.85, density = True )


for i in range(len(thetas)):
      print(pm3.stats.hpd(np.array(accept[i])))        
#plt.title('Theta Histograms')
#plt.xlabel('Calendar years')
#plt.ylabel('Probability density')